package org.opensha.oaf.etas;

//package scratch.aftershockStatisticsETAS;

import java.awt.geom.Point2D;
import java.util.ArrayList;
import java.util.Arrays;
import java.util.Collections;
import java.util.List;

import org.apache.commons.lang3.ArrayUtils;
import org.apache.commons.math3.analysis.UnivariateFunction;
import org.apache.commons.math3.distribution.PoissonDistribution;
import org.apache.commons.math3.optim.MaxEval;
import org.apache.commons.math3.optim.nonlinear.scalar.GoalType;
import org.apache.commons.math3.optim.univariate.BrentOptimizer;
import org.apache.commons.math3.optim.univariate.SearchInterval;
import org.apache.commons.math3.optim.univariate.UnivariateObjectiveFunction;
import org.apache.commons.math3.optim.univariate.UnivariateOptimizer;
import org.apache.commons.math3.optim.univariate.UnivariatePointValuePair;
import org.apache.commons.math3.special.Gamma;
import org.apache.commons.math3.util.DoubleArray;
import org.opensha.commons.data.function.ArbDiscrEmpiricalDistFunc;
import org.opensha.commons.data.function.ArbitrarilyDiscretizedFunc;
import org.opensha.commons.data.function.EvenlyDiscretizedFunc;
import org.opensha.commons.data.function.HistogramFunction;
import org.opensha.commons.data.siteData.impl.TectonicRegime;
import org.opensha.commons.data.xyz.EvenlyDiscrXYZ_DataSet;
import org.opensha.commons.param.impl.DoubleParameter;
import org.opensha.sha.earthquake.observedEarthquake.ObsEqkRupList;
import org.opensha.sha.earthquake.observedEarthquake.ObsEqkRupture;
import org.opensha.sha.gui.infoTools.CalcProgressBar;

/**
 * This represents an ETAS (Ogata 1992, 2006) aftershock model.
 * 
 * This class implements the forecast through an ETAS model 
 * Subclasses should (sometimes) normalize Likelihood values so they sum to 1.0 over the range of parameter-values specified;
 * this can be accomplished using the convertArrayToLikelihood(double maxLogLikeVal) method.
 * 
 * @author field
 * @author van der Elst
 *
 */
public abstract class ETAS_AftershockModel {

	private Boolean D=true;	// debug flag

	protected ArbDiscrEmpiricalDistFunc num_DistributionFunc = null;
	protected CalcProgressBar progress;
	
	// generic/prior model parameters
	protected TectonicRegime regime;
	protected double mean_ams, sigma_ams;
	protected double mean_a, sigma_a;
	protected double mean_p, sigma_p;
	protected double mean_c, sigma_logc;
	protected double b, bSigma;
	protected double alpha, refMag, mu;
	protected double ac; // this is the productivity parameter used for time-dependentMc calculations
	
	// forecast parameters
	protected double dataStartTimeDays, dataEndTimeDays;
	protected double forecastMinDays, forecastMaxDays;

	// sequence data
	protected ObsEqkRupList aftershockList;
	protected ObsEqkRupture mainShock;
	protected double magMain, magComplete; 
	protected double[] magAftershocks;
	protected double[] relativeTimeAftershocks;
	
	// likelihood search parameters
	protected double min_ams, max_ams, delta_ams=0, min_a, max_a, delta_a=0, min_p, max_p, delta_p=0, min_c, max_c, delta_c=0;	//grid search not used for forecast
	protected int num_ams = 51, num_a = 31, num_p = 31, num_c = 21;
	protected double[] ams_vec, a_vec, p_vec, c_vec;
	protected double[][][][] likelihood;
	protected double[][][][] epiLikelihood;
	protected int max_ams_index = -1;
	protected int max_a_index=-1;
	protected int max_p_index=-1;
	protected int max_c_index=-1;

	// simulation parameters
	protected double maxMag;
	protected int maxGenerations;
	protected int nSims;
	protected ETAScatalog simulatedCatalog;	//results of the stochastic simulations
	protected Boolean timeDependentMc = false;
	protected boolean validate;
	
	/**
	 * This converts the likelihood from log-likelihood to likelihood values, making sure NaNs do not occur 
	 * due to high logLikelihoods, and re-normalizes the likelihood so as values sum to 1.0.
	 * 
	 * @param maxLogLikeVal - the maximum values in the input likelihood
	 * @return
	 */
	protected double convertLogLikelihoodArrayToLikelihood(double maxLogLikeVal) {
		double total=0;
		//		double corr = 0;
		//		if(maxLogLikeVal>100.0)	// values above ~700 cause NaNs from Math.exp(), so we subtract some number from all values to avoid such numerical problems
		//			corr = maxLogLikeVal-100;

		double corr = maxLogLikeVal;
		double loglike, like;
		for(int amsIndex=0;amsIndex<num_ams;amsIndex++) {
			for(int aIndex=0;aIndex<num_a;aIndex++) {
				for(int pIndex=0;pIndex<num_p;pIndex++) {
					for(int cIndex=0;cIndex<num_c;cIndex++) {
						loglike = likelihood[amsIndex][aIndex][pIndex][cIndex] - corr;
						if(loglike < -20)
							like = 0;
						else
							like = Math.exp(loglike);

						//						if(Double.isNaN(like))
						//							System.out.println("NaN encountered in likelihood for index [" + amsIndex + " " + aIndex + " " + pIndex + " " + cIndex + "]" );

						likelihood[amsIndex][aIndex][pIndex][cIndex] = like;
						total += likelihood[amsIndex][aIndex][pIndex][cIndex];
					}
				}
			}
		}

		// now re-normalize all likelihood values so they sum to 1.0
		double testTotal=0;
		for(int amsIndex=0;amsIndex<num_ams;amsIndex++) {
			for(int aIndex=0;aIndex<num_a;aIndex++) {
				for(int pIndex=0;pIndex<num_p;pIndex++) {
					for(int cIndex=0;cIndex<num_c;cIndex++) {
						likelihood[amsIndex][aIndex][pIndex][cIndex] /= total;
						testTotal += likelihood[amsIndex][aIndex][pIndex][cIndex];
					}
				}
			}
		}
		return testTotal;
	}


	/**
	 * This converts the likelihood from log-likelihood to likelihood values, making sure NaNs do not occur 
	 * due to high logLikelihoods, but does NOT re-normalize the likelihood so as values sum to 1.0.
	 * 
	 * @param maxLogLikeVal - the maximum values in the input likelihood
	 * @return
	 */
	protected double convertLogLikelihoodArrayToLikelihood_nonNormalized(double[][][][] likelihood, double maxLogLikeVal) {
		double total = 0;
		//		double corr;
		double like, loglike;
		//		if(maxLogLikeVal>100.0)	// values above ~700 cause NaNs from Math.exp(), so we subtract some number from all values to avoid such numerical problems
		//			corr = maxLogLikeVal-100;
		double corr = maxLogLikeVal;
		for(int amsIndex=0;amsIndex<num_ams;amsIndex++) {
			for(int aIndex=0;aIndex<num_a;aIndex++) {
				for(int pIndex=0;pIndex<num_p;pIndex++) {
					for(int cIndex=0;cIndex<num_c;cIndex++) {
						loglike = likelihood[amsIndex][aIndex][pIndex][cIndex]-corr;
						if(loglike < -20)
							like = 0;
						else
							like = Math.exp(loglike);

						likelihood[amsIndex][aIndex][pIndex][cIndex] = like;
						total += likelihood[amsIndex][aIndex][pIndex][cIndex];
					}
				}
			}
		}

		return total;
	}

	public double getMaxLikelihood_ams() { return get_ams(max_ams_index);}

	public double getMaxLikelihood_a() { return get_a(max_a_index);}

	public double getMaxLikelihood_p() { return get_p(max_p_index);}

	public double getMaxLikelihood_c() { return get_c(max_c_index);}

	protected double get_ams(int amsIndex) { return ams_vec[amsIndex];}

	protected double get_a(int aIndex) { return a_vec[aIndex];}

	protected double get_p(int pIndex) { return p_vec[pIndex];}

	protected double get_c(int cIndex) { return c_vec[cIndex];}

	public double getMainShockMag() {return magMain;}

	public double get_b() {return b;}

	public void set_b(double b){ this.b = b; }
	 
	public double get_bSigma() {return bSigma;}
	
	public void set_bSigma(double bSigma){ this.bSigma = bSigma; }

	public void set_alpha(double alpha){ this.alpha = alpha; }

	public double[][][][] getLikelihood() { return likelihood; };

	public double getForecastMinDays(){
		return forecastMinDays;
	};

	public double getForecastMaxDays(){
		return forecastMaxDays;
	};

	public void setRegime(TectonicRegime regime){
		this.regime = regime;
	}

	public TectonicRegime getRegime(){
		return regime;
	}

	public int get_nSims(){
		return nSims;
	}

	public void setMagComplete(double magComplete) {
		double prevMc = this.magComplete;
		this.magComplete = magComplete;
		this.mean_a += Math.log10( (this.maxMag - prevMc)/(this.maxMag - magComplete) );
		this.mean_ams += Math.log10( (this.maxMag - prevMc)/(this.maxMag - magComplete) );
	}
	
	//  additional getters/setters commented out until needed:
	//	public double getMin_a() { return min_a;}
	//	public double getMin_p() { return min_p;}
	//	public double getMin_c() { return min_c;}
	//	public double getMax_a() { return max_a;}
	//	public double getMax_p() { return max_p;}
	//	public double getMax_c() { return max_c;}
	//	public double getDelta_a() { return delta_a;}
	//	public double getDelta_p() { return delta_p;}
	//	public double getDelta_c() { return delta_c;}
	//	public int getNum_a() { return num_a;}
	//	public int getNum_p() { return num_p;}
	//	public int getNum_c() { return num_c;}

	/**
	 * This computes the distribution of the number of Mâ‰¥forecastMag events for the suite of simulated ETAS catalogs
	 * @param tMinDays
	 * @param tMaxDays
	 * @param forecastMag
	 */
	public ArbDiscrEmpiricalDistFunc computeNum_DistributionFunc(double tMinDays, double tMaxDays, double forecastMag) {

		num_DistributionFunc = new ArbDiscrEmpiricalDistFunc();

		int[] numM = new int[simulatedCatalog.nSims];

		List<float[]> eqCat = new ArrayList<float[]>();

		Point2D pt = new Point2D.Double();

		//cycle through the simulated catalogs
		for(int i = 0; i < simulatedCatalog.nSims; i++){
			eqCat = simulatedCatalog.getETAScatalog(i); 	//double[] eqCat = {relativeTime, magnitude, generationNumber}
			numM[i] = 0;
			//count all events in time window and magnitude range in this catalog
			for(float[] eq : eqCat){
				if(eq[0] > tMinDays && eq[0] <= tMaxDays && eq[1] >= forecastMag)
					numM[i] ++;
			}
			
//			for(float[] eq : eqCat){
//				if(eq[0] > tMinDays && eq[0] <= tMaxDays)
//					numM[i] ++;
//			}
//			numM[i] = (int)(numM[i] * Math.pow(10, -b*(forecastMag - magComplete)) + 0.5); //scale to forecast mag (instead of using mags from stochastic catalogs)	
			
			pt.setLocation(numM[i], 1d/simulatedCatalog.nSims);
			num_DistributionFunc.set(pt);	//increment the distribution
		}
		return num_DistributionFunc;
	}

	/**
	 * This gives the number of aftershocks associated with the maximum likelihood a/p/c parameters (which represents the mode
	 * in the number of events space) above the given minimum magnitude and over the specified site span.  A GR distribution with
	 * no upper bound is assumed.
	 * @param magMin
	 * @param tMinDays
	 * @param tMaxDays
	 * @return
	 */
	public double getModalNumEvents(double magMin, double tMinDays, double tMaxDays) {
		//		num_DistributionFunc = computeNum_DistributionFunc( tMinDays,  tMaxDays,  magMin);
		return getFractileNumEvents(magMin, tMinDays, tMaxDays, 0.5);
	}

	public double getFractileNumEvents(double magMin, double tMinDays, double tMaxDays, double fractile) {
		num_DistributionFunc = computeNum_DistributionFunc( tMinDays,  tMaxDays,  magMin);
		if (num_DistributionFunc.size() > 0)
			return num_DistributionFunc.getDiscreteFractile(fractile);
		else
			return 0;
	}

	public void generateStochasticCatalog(double dataMinDays, double dataMaxDays, double forecastMinDays, double forecastMaxDays, int nSims){
		generateStochasticCatalog( dataMinDays,  dataMaxDays,  forecastMinDays,  forecastMaxDays,  nSims, true);
	
	}
	
	public void generateStochasticCatalog(double dataMinDays, double dataMaxDays, double forecastMinDays, double forecastMaxDays, int nSims, boolean dynamicMinMag){
		if(D){
			System.out.println("Computing Forecast with " + nSims + " simulations (Data window: " + dataMinDays +" "+ dataMaxDays + ", Forecast window: "+ forecastMinDays +" "+ forecastMaxDays + ")");
			System.out.println("Model Params: "+ getMaxLikelihood_ams() +" "+ getMaxLikelihood_a() +" "+ getMaxLikelihood_p() +" "+ getMaxLikelihood_c() +" "+ alpha +" "+ b +" "+ magComplete + " " + refMag);
		}

		ETAScatalog simulatedCatalog;

		// make sure that there are a few magnitude units worth of simulated earthquakes
		double maxASmag = magMain;
		for (int i = 0; i < magAftershocks.length; i++) {
			if (magAftershocks[i] > maxASmag)
				maxASmag = magAftershocks[i];
		}
		
		double minMag;
		if (dynamicMinMag) {
			minMag = maxASmag - 2.0;


			// calibrate minimum magnitude choice by running 100 sims. If any come back with zero events, reduce minMag
			if (nSims > 0) {
				int nCalibrationSims = 100;
				boolean iszero = true;
				while (iszero & minMag > 3.0){
					simulatedCatalog = new ETAScatalog(ams_vec, a_vec, p_vec, c_vec, epiLikelihood, alpha, b, refMag, 
							mainShock, aftershockList, dataMinDays, dataMaxDays, forecastMinDays, forecastMaxDays, magComplete, minMag, maxMag, maxGenerations, nCalibrationSims, validate); //maxMag = 9.5, maxGeneratons = 100;
					iszero = false;
					for (int i = 0; i < simulatedCatalog.numEventsFinal.length; i++) {
						if(simulatedCatalog.numEventsFinal[i] == 0)
							iszero = true;
					}
					minMag--;
				}

				if (minMag < 3) minMag = 3;
			}
		} else {
			minMag = 3;
		}

		// now make the real catalogs with the new minMag, etc.
		try{
			simulatedCatalog = new ETAScatalog(ams_vec, a_vec, p_vec, c_vec, epiLikelihood, alpha, b, refMag, 
					mainShock, aftershockList, dataMinDays, dataMaxDays, forecastMinDays, forecastMaxDays, magComplete, minMag, maxMag, maxGenerations, nSims, validate); //maxMag = 9.5, maxGeneratons = 100;
		} catch(Exception e) {
			e.printStackTrace();
			System.err.println("The Java Virtual Machine may have run out of memory.\n"
			+" Increase Mc by one unit and try calculating the forecast again.");
			simulatedCatalog = null;
		}

		this.forecastMinDays = forecastMinDays;
		this.forecastMaxDays = forecastMaxDays;
		this.simulatedCatalog = simulatedCatalog;
		this.nSims = nSims;
		
 		
	}

	
	
	public ArbitrarilyDiscretizedFunc getModalCumNumEventsWithLogTime(double magMin, double tMinDays, double tMaxDays, int numPts) {
		return getFractileCumNumEventsWithLogTime(magMin,tMinDays,tMaxDays,numPts,0.5);
	}


	// get forecast number from simulation
	public ArbitrarilyDiscretizedFunc getFractileCumNumEventsWithLogTime(double magMin, double tMinDays, double tMaxDays, int numPts, double fractile) {

		ArbitrarilyDiscretizedFunc cumFunc = new ArbitrarilyDiscretizedFunc();

		double count = 0, obsCount = 0;

		// set up vector of times
		double[] tvec;
		if (tMinDays > 1e-3){
			tvec = ETAS_StatsCalc.logspace(tMinDays, tMaxDays, numPts);
		} else if (tMaxDays > 1e-3){
			tvec = ETAS_StatsCalc.logspace(1e-3, tMaxDays, numPts);
		} else {
			tvec = ETAS_StatsCalc.linspace(tMinDays, tMaxDays, numPts);
		}

		// get number of events observed prior to forecastWindow
		ObsEqkRupList subList = aftershockList.getRupsAboveMag(magMin)
				.getRupsBefore((long)(mainShock.getOriginTime() + forecastMinDays*ETAS_StatsCalc.MILLISEC_PER_DAY));
		obsCount = subList.size();

		//this is pretty damn slow. Can I speed it up?
		for(double t : tvec){
			count = getFractileNumEvents(magMin,  tMinDays,  t, fractile);
			cumFunc.set(t, obsCount + count);
		}
		return cumFunc;
	}

	
	
	
	// get expected number (straight from the ETAS model)
	public ArbitrarilyDiscretizedFunc getExpectedNumEventsWithLogTime(double magMin, double tMinDays, double tMaxDays, int numPts) {

		ArbitrarilyDiscretizedFunc cumFunc = new ArbitrarilyDiscretizedFunc();

		double expCount = 0;

		// get number of events observed prior to forecastWindow
		ObsEqkRupList subList = aftershockList.getRupsAboveMag(magMin)
				.getRupsBefore((long)(mainShock.getOriginTime() + tMinDays*ETAS_StatsCalc.MILLISEC_PER_DAY));
		int baseCount = subList.size();
		
		// get number of aftershocks in forecastWindow
		subList = aftershockList.getRupsAboveMag(magMin)
				.getRupsBetween((long)(tMinDays*ETAS_StatsCalc.MILLISEC_PER_DAY), (long)(tMaxDays*ETAS_StatsCalc.MILLISEC_PER_DAY));
		
		double[] tvec, tas, tvecLog, tvecLin;
		
		tas = ETAS_StatsCalc.getDaysSinceMainShockArray(mainShock, subList);
		int totalCount = tas.length;
		
		// use a minimum of numPts pts, but start with the times of the actual aftershocks
		if (totalCount < numPts) {
			// pad it with log and lin-spaced points
			if(D) System.out.println("padding time vector");
			int remainingCount = numPts - totalCount;
			int logCount = remainingCount/2;
			int linCount = remainingCount - logCount;
			
			if (tMinDays > 1e-3){
				tvecLog = ETAS_StatsCalc.logspace(tMinDays, tMaxDays, logCount);
			} else if (tMaxDays > 1e-3){
				tvecLog = ETAS_StatsCalc.logspace(1e-3, tMaxDays, logCount);
			} else {
				tvecLog = new double[0];
				linCount = remainingCount;
			}
			
			tvecLin = ETAS_StatsCalc.linspace(tMinDays, tMaxDays, linCount);
			
			tvec = ArrayUtils.addAll(ArrayUtils.addAll(tas, tvecLin), tvecLog);
			Arrays.sort(tvec);
		} else {
			//randomly thin the aftershock list
			if(D) System.out.println("thinning time vector");
			Collections.shuffle(Arrays.asList(tas));
			tvec = ArrayUtils.subarray(tas, 0, numPts);
		}

//		
//		if (tMinDays > 1e-3){
//			tvec = ETAS_StatsCalc.logspace(tMinDays, tMaxDays, numPts);
//		} else if (tMaxDays > 1e-3){
//			tvec = ETAS_StatsCalc.logspace(1e-3, tMaxDays, numPts);
//		} else {
//			tvec = ETAS_StatsCalc.linspace(tMinDays, tMaxDays, numPts);
//		}
		
		for(double t : tvec){
			//compute expected number for comparison with known quakes (plot fit)
			expCount = getExpectedNumEvents(magMin, tMinDays, t);
			cumFunc.set(t, baseCount + expCount);
		}
		return cumFunc;
	}

	// get expected number (straight from the ETAS model)
	public double getExpectedNumEvents(double magMin, double tMinDays, double tMaxDays){

		double[] relativeTimeAftershocks = new double[this.relativeTimeAftershocks.length];
		double[] magAftershocks = new double[this.magAftershocks.length];

		int Nas = 0;
		for(int i = 0; i < magAftershocks.length; i++){
			if(this.magAftershocks[i] >= magMin){
				magAftershocks[Nas] = this.magAftershocks[i];
				relativeTimeAftershocks[Nas] = this.relativeTimeAftershocks[i];
				Nas++;
			}
		}
		relativeTimeAftershocks = Arrays.copyOf(relativeTimeAftershocks, Nas);
		magAftershocks = Arrays.copyOf(magAftershocks, Nas);

		// now compute numbers
		double ams = getMaxLikelihood_ams();
		double a = getMaxLikelihood_a();
		double p = getMaxLikelihood_p();
		double c = getMaxLikelihood_c();
//		double k = Math.pow(10, ams + alpha*(mainShock.getMag() - refMag) + b*(refMag - magComplete));
		double k = Math.pow(10, ams + alpha*(mainShock.getMag() - magComplete)); //update 4/2/18
		double kc, cms;
		
		double timeIntegral;

		//compute total number at end of window due to mainshock
		double Ntot;
		if (timeDependentMc) {
			kc = Math.pow(10, ac-ams);
			cms = c*Math.pow(k*kc,1d/p);
			if (Math.abs(1-p) < 1e-6) {
				timeIntegral = Math.log(tMaxDays + cms) - Math.log(tMinDays + cms);
			} else {
				timeIntegral = (Math.pow(tMaxDays + cms, 1d-p) - Math.pow(tMinDays + cms, 1d-p)) / (1d-p);
			}
		} else {
			if (Math.abs(1-p) < 1e-6) 
				timeIntegral = Math.log(tMaxDays + c) - Math.log(tMinDays + c);
			else
				timeIntegral = (Math.pow(tMaxDays + c, 1d-p) - Math.pow(tMinDays + c, 1d-p)) / (1d-p);
		}
		Ntot = k*timeIntegral;		//mainshock contribution
		
		
		double[] productivity = new double[Nas];

		kc = Math.pow(10, ac-a);
		for(int i=0; i<Nas; i++){
			//compute productivity for this aftershock
//			productivity[i] = Math.pow(10, a + alpha*(magAftershocks[i] - refMag) + b*(refMag - magComplete));
			productivity[i] = Math.pow(10, a + alpha*(magAftershocks[i] - magComplete));	//update 4/2/18

			//compute number at end of window due to this aftershock
			if(relativeTimeAftershocks[i] <= tMaxDays){
				if (timeDependentMc) {
					cms = c*Math.pow(kc*productivity[i],1d/p);
					if(Math.abs(1-p) < 1e-6) {
						timeIntegral = Math.log(tMaxDays - relativeTimeAftershocks[i] + cms) - Math.log(cms); 
					} else {
						timeIntegral = (Math.pow(tMaxDays - relativeTimeAftershocks[i] + cms, 1d-p) - Math.pow(cms, 1d-p)) / (1d-p);
					}
					Ntot += productivity[i]*timeIntegral;	//aftershock Contributions
				} else {
					if(Math.abs(1-p) < 1e-6)
						timeIntegral = Math.log(tMaxDays - relativeTimeAftershocks[i] + c) - Math.log(c); 
					else
						timeIntegral = (Math.pow(tMaxDays - relativeTimeAftershocks[i] + c, 1d-p) - Math.pow(c, 1d-p)) / (1d-p);

					Ntot += productivity[i]*timeIntegral;	//aftershock Contributions
				}

			}
		}

		return Ntot;
	}

//	public EvenlyDiscretizedFunc[] getCumNumMFD_FractileWithAleatoryVariability(double[] fractileArray, double minMag, double maxMag, int numMag, double tMinDays, double tMaxDays) {
//		EvenlyDiscretizedFunc[] mfdArray = new EvenlyDiscretizedFunc[fractileArray.length];
//
//		for(int i=0;i<fractileArray.length;i++) {
//			mfdArray[i] = new EvenlyDiscretizedFunc(minMag, maxMag, numMag);
//			mfdArray[i].setName(fractileArray[i]+" Fractile for Num Events, including aleatory variability");
//			mfdArray[i].setInfo("Cumulative distribution (greater than or equal to each magnitude)");
//		}
//
//		// get fractile rates at magComplete
//		//		double mag0 = mfdArray[0].getX(0);	// any MFD will do, as they all have the same x-axis values
////		double mag0 = magComplete;
////		double[] valsArray0 = getCumNumFractileWithAleatory(fractileArray, mag0, tMinDays, tMaxDays);
////
////		double mag, val;
////		for(int i=0;i<numMag;i++) {
////			mag = mfdArray[0].getX(i);	// any MFD will do, as they all have the same x-axis values
////			for(int j=0;j<fractileArray.length;j++) {
////				val = valsArray0[j]*Math.pow(10, -b*(mag - mag0));
////				mfdArray[j].set(i,val);
////			}
////		}
//		
//		// use the stochastic method
////		computeNum_DistributionFunc(tMinDays, tMaxDays, minMag);
//		int[] eventCounts = simulatedCatalog.getEventCounts(tMinDays,tMaxDays,minMag);
//		StackedMND mndStack = new StackedMND(mfdArray, eventCounts, magComplete); 
//		mndStack.generateQuantiles(b, fractileArray);
//		
//		return mndStack.getQuantiles();
//		
//	}
	
//	public List<EvenlyDiscretizedFunc[]> getMNDandMPOE(double[] fractileArray, double minMag, double maxMag, int numMag, double tMinDays, double tMaxDays) {
//		List<EvenlyDiscretizedFunc[]> forecast = new ArrayList<EvenlyDiscretizedFunc[]>();
//		
//		EvenlyDiscretizedFunc[] mfdArray = new EvenlyDiscretizedFunc[fractileArray.length];
//
//		for(int i=0;i<fractileArray.length;i++) {
//			mfdArray[i] = new EvenlyDiscretizedFunc(minMag, maxMag, numMag);
//			mfdArray[i].setName(fractileArray[i]+" Fractile for Num Events, including aleatory variability");
//			mfdArray[i].setInfo("Cumulative distribution (greater than or equal to each magnitude)");
//		}
//
//		EvenlyDiscretizedFunc[] magnitudePDF = new EvenlyDiscretizedFunc[1];
//		magnitudePDF[0] = new EvenlyDiscretizedFunc(minMag, maxMag, numMag);
//		
//		magnitudePDF[0].setName("probabiltiy of exceeding magnitude M");
//		magnitudePDF[0].setInfo("Cumulative distribution (greater than or equal to each magnitude)");
//		
//		// use the stochastic method
//		int[] eventCounts = simulatedCatalog.getEventCounts(tMinDays,tMaxDays,minMag);
//		double minMagSim = simulatedCatalog.minMagLimit;
//		StackedMND mndStack = new StackedMND(mfdArray, eventCounts, minMagSim, b, fractileArray); 
//		forecast.add(mndStack.getQuantiles());
//
//		// return magnitude probabilities
//		magnitudePDF[0] = mndStack.getProbabilities();
//		forecast.add(magnitudePDF);
//		return forecast;
//	}
//
//	public double[] getCumNumFractileWithAleatory(double[] fractileArray, double mag, double tMinDays, double tMaxDays) {
//		// use the stochastic method. This needs a new constructor.
//		double[] fractValArray = new double[fractileArray.length];
//		int[] eventCounts = simulatedCatalog.getEventCounts(tMinDays,tMaxDays,mag);
//		double minMagSim = simulatedCatalog.minMagLimit;
//		StackedMND mndStack = new StackedMND(mag, eventCounts, minMagSim, b, fractileArray); 
//		fractValArray = mndStack.getSingleQuantile();
//
//		return fractValArray;
//	}
	
//	public List<EvenlyDiscretizedFunc[]> getMNDandMPOE(double[] fractileArray, double minMag, double maxMag, int numMag, double tMinDays, double tMaxDays) {
//		List<EvenlyDiscretizedFunc[]> forecast = new ArrayList<EvenlyDiscretizedFunc[]>();
//		
//		EvenlyDiscretizedFunc[] mfdArray = new EvenlyDiscretizedFunc[fractileArray.length];
//
//		for(int i=0;i<fractileArray.length;i++) {
//			mfdArray[i] = new EvenlyDiscretizedFunc(minMag, maxMag, numMag);
//			mfdArray[i].setName(fractileArray[i]+" Fractile for Num Events, including aleatory variability");
//			mfdArray[i].setInfo("Cumulative distribution (greater than or equal to each magnitude)");
//		}
//
//		EvenlyDiscretizedFunc[] magnitudePDF = new EvenlyDiscretizedFunc[1];
//		magnitudePDF[0] = new EvenlyDiscretizedFunc(minMag, maxMag, numMag);
//		
//		magnitudePDF[0].setName("probabiltiy of exceeding magnitude M");
//		magnitudePDF[0].setInfo("Cumulative distribution (greater than or equal to each magnitude)");
//		
//		// use the stochastic method
//		int[] eventCounts = simulatedCatalog.getEventCounts(tMinDays,tMaxDays,minMag);
//		double minMagSim = simulatedCatalog.minMagLimit;
//		StackedMND mndStack = new StackedMND(mfdArray, eventCounts, minMagSim, b, fractileArray); 
//		forecast.add(mndStack.getQuantiles());
//
//		// return magnitude probabilities
//		magnitudePDF[0] = mndStack.getProbabilities();
//		forecast.add(magnitudePDF);
//		return forecast;
//	}

	public List<EvenlyDiscretizedFunc[]> getMNDandMPOE(double[] fractileArray, double minMag, double maxMag, int numMag, double tMinDays, double tMaxDays) {
		List<EvenlyDiscretizedFunc[]> forecast = new ArrayList<EvenlyDiscretizedFunc[]>();
		
		EvenlyDiscretizedFunc[] mfdArray = new EvenlyDiscretizedFunc[fractileArray.length];

		for(int i=0;i<fractileArray.length;i++) {
			mfdArray[i] = new EvenlyDiscretizedFunc(minMag, maxMag, numMag);
			mfdArray[i].setName(fractileArray[i]+" Fractile for Num Events, including aleatory variability");
			mfdArray[i].setInfo("Cumulative distribution (greater than or equal to each magnitude)");
		}

		EvenlyDiscretizedFunc[] magnitudePDF = new EvenlyDiscretizedFunc[1];
		magnitudePDF[0] = new EvenlyDiscretizedFunc(minMag, maxMag, numMag);
		
		magnitudePDF[0].setName("probabiltiy of exceeding magnitude M");
		magnitudePDF[0].setInfo("Cumulative distribution (greater than or equal to each magnitude)");
		
		for (int i = 0; i < mfdArray[0].size(); i++) {
			magnitudePDF[0].set(i, getProbabilityWithAleatory(mfdArray[0].getX(i), tMinDays, tMaxDays));
		}
		
		// use the stochastic method
//		int[] eventCounts = simulatedCatalog.getEventCounts(tMinDays,tMaxDays,minMag);
//		double minMagSim = simulatedCatalog.minMagLimit;
//		StackedMND mndStack = new StackedMND(mfdArray, eventCounts, minMagSim, b, fractileArray);
//		forecast.add(mndStack.getQuantiles());
		
		mfdArray = getCumNumMFD_FractileWithAleatoryVariability(fractileArray, minMag, maxMag, numMag, tMinDays, tMaxDays);
		forecast.add(mfdArray);

		// return magnitude probabilities
//		magnitudePDF[0] = mndStack.getProbabilities();

		forecast.add(magnitudePDF);
		return forecast;
	}

	
	
	public EvenlyDiscretizedFunc[] getCumNumMFD_FractileWithAleatoryVariability(double[] fractileArray, double minMag, double maxMag, int numMag, double tMinDays, double tMaxDays) {
		EvenlyDiscretizedFunc[] mfdArray = new EvenlyDiscretizedFunc[fractileArray.length];

		for(int i=0;i<fractileArray.length;i++) {
			mfdArray[i] = new EvenlyDiscretizedFunc(minMag, maxMag, numMag);
			mfdArray[i].setName(fractileArray[i]+" Fractile for Num Events, including aleatory variability");
			mfdArray[i].setInfo("Cumulative distribution (greater than or equal to each magnitude)");
		}

		//		 get fractile rates at magComplete
		double mag0 = mfdArray[0].getX(0);	// any MFD will do, as they all have the same x-axis values
		//				double mag0 = magComplete;
//		double[] valsArray0 = getCumNumFractileWithAleatory(fractileArray, mag0, tMinDays, tMaxDays);
		double[] valsArray0;
		
		double mag, val;
		for(int i=0;i<numMag;i++) {
			mag = mfdArray[0].getX(i);	// any MFD will do, as they all have the same x-axis values
			valsArray0 = getCumNumFractileWithAleatory(fractileArray, mag, tMinDays, tMaxDays);
			for(int j=0;j<fractileArray.length;j++) {
				val = valsArray0[j];
				mfdArray[j].set(i,val);
			}
		}
		return mfdArray;
	}

	public double[] getCumNumFractileWithAleatory(double[] fractileArray, double mag, double tMinDays, double tMaxDays) {
		computeNum_DistributionFunc(tMinDays, tMaxDays, mag);
		double[] fractValArray = new double[fractileArray.length];

		for(int i = 0; i < fractileArray.length; i++){
			double fractValComplete = num_DistributionFunc.getDiscreteFractile(fractileArray[i]);
			
			// fractValComplete is an integer (the number of earthquakes for which f% of the simulations have fewer)
			// this can be zero, which is no good for scaling the rate to magnitudes lower than mag (or magComplete)
			// It is therefore necessary to produce some kind of fractional count. We do this by computing the
			// Poisson rate that gives the observed probability of zero events in the simulations. This is approximate, but
			// do you have a better idea?

//			if(D) System.out.println("mag=" + mag + " duration=" + (tMaxDays-tMinDays) + " fractValComplete=" + fractValComplete);
//			if(fractValComplete <= 1){
//				if(D) System.out.println("using Poisson approximation for fractile");
//				double probOne = 1 - num_DistributionFunc.getY(0); // (1 - probability of zero events)
//				double lambda = -Math.log(1-probOne);
//				fractValComplete = poissQuantile(lambda, fractileArray[i]);
//
//				if(D) System.out.println("new fractValComplete=" + fractValComplete + " probOne=" + probOne + " lambda=" + lambda);
//			}
			// the preceding gives the number of magComplete used in the simulation. We want the number of mag. Scale:
			if(mag < simulatedCatalog.minMagLimit)
				fractValArray[i] = Math.pow(10, -b*(mag - simulatedCatalog.minMagLimit)) * fractValComplete;	
			else
				fractValArray[i] = fractValComplete;
		}

		return fractValArray;
	}
	
	public double getMedianPoissInterp(double mag, double tMinDays, double tMaxDays) {
		computeNum_DistributionFunc(tMinDays, tMaxDays, mag);
		double fractVal, fractValComplete;
		int discreteMedian = (int)(num_DistributionFunc.getDiscreteFractile(0.5) + 0.5);
		
		// find the probability of exceeding the discrete median value (less than 50%, usually)
		double prob = 1;
		for (int x = 0; x <= discreteMedian; x++)
			prob -= num_DistributionFunc.getY(x); // (1 - probability of i events)
				
		// find lambda, using a search
		double lambda = findPoissLambda(prob, discreteMedian);
				
		if(D) System.out.println("new discreteMedian=" + discreteMedian + " prob of exceeding median=" + prob + " lambda=" + lambda);
		fractValComplete = lambda;
			
		// the preceding gives the number of magComplete used in the simulation. We want the number of mag. Scale:
		if(mag < simulatedCatalog.minMagLimit)
			fractVal = Math.pow(10, -b*(mag - simulatedCatalog.minMagLimit)) * fractValComplete;	
		else
			fractVal = fractValComplete;

		return fractVal;
	}
	
	private double findPoissLambda(double prob, int discreteMedian){
		LambdaMisfit lambdaMisfit = new LambdaMisfit();
		lambdaMisfit.setParams(discreteMedian, prob);
		UnivariateOptimizer fminbnd = new BrentOptimizer(1e-6,1e-9);

		UnivariatePointValuePair optimum = fminbnd.optimize(
				new UnivariateObjectiveFunction(lambdaMisfit), new MaxEval(100), GoalType.MINIMIZE,
				new SearchInterval(0,20));
		return optimum.getPoint();
	
	}
	private class LambdaMisfit implements UnivariateFunction{
		private int discreteMedian;
		private double prob;
		
		public double value(double lambda) {
			PoissonDistribution poissDist = new PoissonDistribution(lambda);
			
			return Math.abs((1 - prob) - poissDist.cumulativeProbability(discreteMedian));

		}

		public void setParams(int discreteMedian, double prob){
			this.discreteMedian = discreteMedian;
			this.prob = prob;
		}
	}
	
//	/** Computes the "Poissonian" quantile using a continuous gamma distribution to get non-integer quantiles.
//	 *  This is approximate. Ideas welcomed.
//	 *  
//	 *  @author Nicholas van der Elst
//	 **/
//	private double poissQuantile(double lambda, double fractile){
//		GammaIncInverse gammaIncInverse = new GammaIncInverse();
//		gammaIncInverse.setParams(lambda, fractile);
//		UnivariateOptimizer fminbnd = new BrentOptimizer(1e-6,1e-9);
//
//		UnivariatePointValuePair optimum = fminbnd.optimize(
//				new UnivariateObjectiveFunction(gammaIncInverse), new MaxEval(100), GoalType.MINIMIZE,
//				new SearchInterval(0,20));
//		return optimum.getPoint();
//	}
//
//	private class GammaIncInverse implements UnivariateFunction{
//		private double lambda, fractile;
//
//		public double value(double x) {
//			return Math.abs(fractile - Gamma.regularizedGammaQ(x, lambda));
//			
//		}
//
//		public void setParams(double lambda, double fractile){
//			this.lambda = lambda;
//			this.fractile = fractile;
//		}
//
//	}

//	/** This one gives probability as a function of magnitude for the specified time range
//	 * 
//	 **/
//	public EvenlyDiscretizedFunc getMagnitudePDFwithAleatoryVariability(double minMag, double maxMag, int numMag, double tMinDays, double tMaxDays) {
//		EvenlyDiscretizedFunc magnitudePDF = new EvenlyDiscretizedFunc(minMag, maxMag, numMag);
//		magnitudePDF.setName("probabiltiy of exceeding magnitude M");
//		magnitudePDF.setInfo("Cumulative distribution (greater than or equal to each magnitude)");
//
////		double value = getProbabilityWithAleatory(minMag, tMinDays, tMaxDays);
//		
//		for(int i=0;i<numMag;i++) {
//			double mag = magnitudePDF.getX(i);	// any MFD will do, as they all have the same x-axis values
//			double value = getProbabilityWithAleatory(mag, tMinDays, tMaxDays);
////			double probOne = 1 - Math.pow(1-value, Math.pow(10, -b*(mag-minMag)));
//			
//			magnitudePDF.set(i,value);
//		}
//		
//	
//		
//		return magnitudePDF;
//	}

//	public double[] getProbabilityWithAleatory(int[] number, double mag, double tMinDays, double tMaxDays) {
//		computeNum_DistributionFunc(tMinDays, tMaxDays, mag);
//
//
//		double[] prob = new double[number.length];
//
//		for (int k = 0; k < number.length; k++) {
//			prob[k] = 1 - num_DistributionFunc.getClosestYtoX(number[k]);
//
//			if(D) System.out.println("Prob value = " + 100.0*prob[k]);
//			// the above probability is the fraction of simulations with events above max(magComplete, mag),
//			// so if mag<magComplete, we need to scale up the probability. We do this with a Poisson rate assumption.
//
//			if(mag < simulatedCatalog.minMagLimit){
//				prob[k] = 1 - Math.pow(1-prob[k], Math.pow(10, -b*(mag-simulatedCatalog.minMagLimit)));
//			}
//
//		}
//		
//		return prob;
//	}


	public double getProbabilityWithAleatory(double mag, double tMinDays, double tMaxDays) {
		
		computeNum_DistributionFunc(tMinDays, tMaxDays, mag);

		double probOne = 1 - num_DistributionFunc.getY(0);

		// the above probability is the fraction of simulations with events above max(magComplete, mag),
		// so if mag<magComplete, we need to scale up the probability. We do this with a Poisson rate assumption.
//		if(mag < magComplete){
//			probOne = 1 - Math.pow(1-probOne, Math.pow(10, -b*(mag-magComplete)));
//		}
		if(mag < simulatedCatalog.minMagLimit){
			probOne = 1 - Math.pow(1-probOne, Math.pow(10, -b*(mag-simulatedCatalog.minMagLimit)));
		}
		
		
		return probOne;
				
		
	}

	/**
	 * This returns the PDF of ams, which is a marginal distribution if either c or p 
	 * are unconstrained (either num_p or num_c not equal to 1). 
	 * @return
	 */
	public HistogramFunction getPDF_ams() {
		if(num_ams == 0) {
			// changed this to still return a 1-bar histogram.
			return null;
		} else {
			if(D) System.out.println("ams: " + getMaxLikelihood_ams() + " " + sigma_ams + " " + min_ams + " " + max_ams + " " + num_ams);
			
			HistogramFunction hist = new HistogramFunction(min_ams, max_ams, num_ams);

			for(int amsIndex=0;amsIndex<num_ams;amsIndex++) {
				for(int aIndex=0;aIndex<num_a;aIndex++) {
					for(int pIndex=0;pIndex<num_p;pIndex++) {
						for(int cIndex=0;cIndex<num_c;cIndex++) {
							hist.add(amsIndex,  likelihood[amsIndex][aIndex][pIndex][cIndex]);
						}
					}
				}
			}

			String name = "PDF of ams-value";
			if(num_ams == 1)
				name += " (constrained)";
			else if(num_p !=1 || num_c != 1 || num_a != 1)
				name += " (marginal)";
			
			hist.setName(name);
//			if(D) System.out.println("PDF of ams-value: totalTest = "+hist.calcSumOfY_Vals());

			hist.scale(1d/hist.getDelta());
			return hist;
		}
	}

	/**
	 * This returns the PDF of a, which is a marginal distribution if either c or p 
	 * are unconstrained (either num_p or num_c not equal to 1). Null is returned if
	 * a is constrained (num_a=1). (how important is this? I want to show pdf with one value)
	 * @return
	 */
	public HistogramFunction getPDF_a() {
		if(num_a == 0) {
			return null;
		}
		else {
			
			if(D) System.out.println("a: " + getMaxLikelihood_a() + " " + sigma_a + " " + min_a + " " + max_a + " " + num_a);
			HistogramFunction hist = new HistogramFunction(min_a, max_a, num_a);
			for(int amsIndex=0;amsIndex<num_ams;amsIndex++) {
				for(int aIndex=0;aIndex<num_a;aIndex++) {
					for(int pIndex=0;pIndex<num_p;pIndex++) {
						for(int cIndex=0;cIndex<num_c;cIndex++) {
							hist.add(aIndex,  likelihood[amsIndex][aIndex][pIndex][cIndex]);
						}
					}
				}
			}
			String name = "PDF of a-value";
			if(num_a == 1)
				name += " (constrained)"; 
			else if(num_p !=1 || num_c != 1)
				name += " (marginal)";
			hist.setName(name);
//			if(D) System.out.println("PDF of a-value: totalTest = "+hist.calcSumOfY_Vals());

			hist.scale(1d/hist.getDelta());
			return hist;
		}
	}

	/**
	 * This returns the PDF of p, which is a marginal distribution if either a or c 
	 * are unconstrained (either num_a or num_c not equal to 1). Null is returned if
	 * p is constrained (num_p=1).
	 * @return
	 */
	public HistogramFunction getPDF_p() {
		if(num_p == 0) {
			return null;
		}
		else {
			if(D) System.out.println("p: " + getMaxLikelihood_p() + " " + sigma_p + " " + min_p + " " + max_p + " " + num_p);
			
			HistogramFunction hist = new HistogramFunction(min_p, max_p, num_p);
			for(int amsIndex=0;amsIndex<num_ams;amsIndex++) {
				for(int aIndex=0;aIndex<num_a;aIndex++) {
					for(int pIndex=0;pIndex<num_p;pIndex++) {
						for(int cIndex=0;cIndex<num_c;cIndex++) {
							hist.add(pIndex, likelihood[amsIndex][aIndex][pIndex][cIndex]);
						}
					}
				}
			}
			String name = "PDF of p-value";
			if(num_p == 1)
				name += " (constrained)";
			else if(num_a !=1 || num_c != 1)
				name += " (marginal)";
			hist.setName(name);
//			if(D) System.out.println("PDF of p-value: totalTest = "+hist.calcSumOfY_Vals() + " Max = " + hist.getMaxY());

			hist.scale(1d/hist.getDelta());
			return hist;
		}
	}

	/**
	 * This returns the PDF of c, which is a marginal distribution if either a or p 
	 * are unconstrained (either num_a or num_p not equal to 1). Null is returned if
	 * c is constrained (num_c=1).
	 * @return
	 */
	public HistogramFunction getPDF_c() {
		if(num_c == 0) {
			return null;
		}
		else {
			if(D) System.out.println("c: " + getMaxLikelihood_c() + " " + sigma_logc + " " + min_c + " " + max_c + " " + num_c);
			
			HistogramFunction hist = new HistogramFunction(min_c, max_c, num_c);
			for(int amsIndex=0;amsIndex<num_ams;amsIndex++) {
				for(int aIndex=0;aIndex<num_a;aIndex++) {
					for(int pIndex=0;pIndex<num_p;pIndex++) {
						for(int cIndex=0;cIndex<num_c;cIndex++) {
							hist.add(cIndex, likelihood[amsIndex][aIndex][pIndex][cIndex]);
						}
					}
				}
			}
			String name = "PDF of c-value";
			if(num_c == 1)
				name += " (constrained)";
			else if(num_a !=1 || num_p != 1)
				name += " (marginal)";
			hist.setName(name);
//			if(D) System.out.println("PDF of c-value: totalTest = "+hist.calcSumOfY_Vals());

			hist.scale(1d/hist.getDelta());
			return hist;
		}
	}


	/**
	 * This returns the PDF of logc, which is a marginal distribution if either a or p 
	 * are unconstrained (either num_a or num_p not equal to 1). Null is returned if
	 * c is constrained (num_c=1).
	 * @return
	 */
	public HistogramFunction getPDF_logc() {
		if(num_c == 0) {
			return null;
		}
		else {
			double min_logc = Math.log10(min_c);
			double max_logc = Math.log10(max_c);

			if(D) System.out.println("c: " + getMaxLikelihood_c() + " " + sigma_logc + " " + min_logc + " " + max_logc + " " + num_c);
			HistogramFunction hist = new HistogramFunction(min_logc, max_logc, num_c);
			
			for(int amsIndex=0;amsIndex<num_ams;amsIndex++) {
				for(int aIndex=0;aIndex<num_a;aIndex++) {
					for(int pIndex=0;pIndex<num_p;pIndex++) {
						for(int cIndex=0;cIndex<num_c;cIndex++) {
							hist.add(cIndex, likelihood[amsIndex][aIndex][pIndex][cIndex]);
						}
					}
				}
			}
			String name = "PDF of logc-value";
			if(num_c == 1)
				name += " (constrained)";
			else if(num_a !=1 || num_p != 1)
				name += " (marginal)";
			hist.setName(name);
//			if(D) System.out.println("PDF of logc-value: totalTest = "+hist.calcSumOfY_Vals());

			hist.scale(1d/hist.getDelta());
			return hist;
		}
	}



	/**
	 * This returns a 2D PDF for a and p, which is a marginal distribution if c 
	 * is unconstrained (num_c not equal to 1). Null is returned if either
	 * a or p are constrained (num_a=1 or num_p=1).
	 * @return
	 */
	public EvenlyDiscrXYZ_DataSet get2D_PDF_for_a_and_p() {
		if(num_a == 1 || num_p == 1) {
			return null;
		}
		else {
			double delta_a = (max_a - min_a)/((double)num_a - 1);
			double delta_p = (max_p - min_p)/((double)num_p - 1);

			EvenlyDiscrXYZ_DataSet hist2D = new EvenlyDiscrXYZ_DataSet(num_a, num_p, min_a, min_p, delta_a, delta_p);
			for(int amsIndex=0;amsIndex<num_ams;amsIndex++) {
				for(int aIndex=0;aIndex<num_a;aIndex++) {
					for(int pIndex=0;pIndex<num_p;pIndex++) {
						for(int cIndex=0;cIndex<num_c;cIndex++) {
							double prevVal = hist2D.get(aIndex,pIndex);
							hist2D.set(aIndex,pIndex, prevVal+likelihood[amsIndex][aIndex][pIndex][cIndex]);
						}
					}
				}
			}
			//			String name = "2D PDF of a vs p";
			//			if(num_c != 1)
			//				name += " (marginal)";
			if(D) {
				System.out.println("2D PDF of a vs p: totalTest = "+hist2D.getSumZ());
			}
			hist2D.scale(1d/(hist2D.getGridSpacingX()*hist2D.getGridSpacingY()));
			return hist2D;
		}
	}


	/**
	 * This returns a 2D PDF for a and c, which is a marginal distribution if p 
	 * is unconstrained (num_p not equal to 1). Null is returned if either
	 * a or c are constrained (num_a=1 or num_c=1).
	 * this one isn't ever used because c is log-normal. Use get2D_PDF_for_a_and_logc() instead.
	 * @return
	 */
	public EvenlyDiscrXYZ_DataSet get2D_PDF_for_a_and_c() {
		if(num_a == 1 || num_c == 1) {
			return null;
		}
		else {
			double delta_c = (max_c - min_c)/((double)num_c - 1);
			double delta_a = (max_a - min_a)/((double)num_a - 1);

			EvenlyDiscrXYZ_DataSet hist2D = new EvenlyDiscrXYZ_DataSet(num_a, num_c, min_a, min_c, delta_a, delta_c);
			for(int amsIndex=0;amsIndex<num_ams;amsIndex++) {
				for(int aIndex=0;aIndex<num_a;aIndex++) {

					for(int pIndex=0;pIndex<num_p;pIndex++) {
						for(int cIndex=0;cIndex<num_c;cIndex++) {
							double prevVal = hist2D.get(aIndex,cIndex);
							hist2D.set(aIndex,cIndex, prevVal+likelihood[amsIndex][aIndex][pIndex][cIndex]);
						}
					}
				}
			}

			if(D) {
				System.out.println("2D PDF of a vs c: totalTest = "+hist2D.getSumZ());
			}
			hist2D.scale(1d/(hist2D.getGridSpacingX()*hist2D.getGridSpacingY()));
			return hist2D;
		}
	}

	/**
	 * This returns a 2D PDF for a and c, which is a marginal distribution if p 
	 * is unconstrained (num_p not equal to 1). Null is returned if either
	 * a or c are constrained (num_a=1 or num_c=1).
	 * @return
	 */
	public EvenlyDiscrXYZ_DataSet get2D_PDF_for_a_and_logc() {
		if(num_a == 1 || num_c == 1) {
			return null;
		}
		else {
			double min_logc = Math.log10(min_c);
			double max_logc = Math.log10(max_c);
			double delta_logc = (max_logc - min_logc)/num_c;
			double delta_a = (max_a - min_a)/((double)num_a - 1);

			EvenlyDiscrXYZ_DataSet hist2D = new EvenlyDiscrXYZ_DataSet(num_a, num_c, min_a, min_logc, delta_a, delta_logc);
			for(int amsIndex=0;amsIndex<num_ams;amsIndex++) {
				for(int aIndex=0;aIndex<num_a;aIndex++) {
					for(int pIndex=0;pIndex<num_p;pIndex++) {
						for(int cIndex=0;cIndex<num_c;cIndex++) {
							double prevVal = hist2D.get(aIndex,cIndex);
							hist2D.set(aIndex,cIndex, prevVal+likelihood[amsIndex][aIndex][pIndex][cIndex]);
						}
					}
				}
			}
			if(D) {
				System.out.println("2D PDF of a vs logc: totalTest = "+hist2D.getSumZ());
			}
			hist2D.scale(1d/(hist2D.getGridSpacingX()*hist2D.getGridSpacingY()));
			return hist2D;
		}
	}

	/**
	 * This returns a 2D PDF for c and p, which is a marginal distribution if a 
	 * is unconstrained (num_a not equal to 1). Null is returned if either
	 * c or p are constrained (num_c=1 or num_p=1).
	 * @return
	 */
	public EvenlyDiscrXYZ_DataSet get2D_PDF_for_logc_and_p() {
		if(num_c == 1 || num_p == 1) {
			return null;
		}
		else {
			double min_logc = Math.log10(min_c);
			double max_logc = Math.log10(max_c);
			double delta_logc = (max_logc - min_logc)/((double)num_c - 1);
			double delta_p = (max_p - min_p)/((double)num_p - 1);

			EvenlyDiscrXYZ_DataSet hist2D = new EvenlyDiscrXYZ_DataSet(num_c, num_p, min_logc, min_p, delta_logc, delta_p);
			for(int amsIndex=0;amsIndex<num_ams;amsIndex++) {
				for(int aIndex=0;aIndex<num_a;aIndex++) {
					for(int pIndex=0;pIndex<num_p;pIndex++) {
						for(int cIndex=0;cIndex<num_c;cIndex++) {
							double prevVal = hist2D.get(cIndex,pIndex);
							hist2D.set(cIndex,pIndex, prevVal+likelihood[amsIndex][aIndex][pIndex][cIndex]);
						}
					}
				}
			}
			if(D) {
				System.out.println("2D PDF of logc vs p: totalTest = "+hist2D.getSumZ());
			}
			hist2D.scale(1d/(hist2D.getGridSpacingX()*hist2D.getGridSpacingY()));
			return hist2D;
		}
	}

	
	public double[] getCumulativeProbabilityValue(double tmin, double tmax, double minMag, int[] nObserved) {
		
		this.computeNum_DistributionFunc(tmin, tmax, minMag);

		
		double[] xValues = num_DistributionFunc.getXVals();
		double[] yValues = num_DistributionFunc.getYVals();
		double[] zValues = new double[nObserved.length];
		
		for (int k = 0; k < nObserved.length; k++) { 
			double ySum = 0;
			
			for (int i = 0; i < xValues.length; i++) {
				if (nObserved[k] >= xValues[i])
					ySum += yValues[i];
				else 
					break;
			}
			
			zValues[k] = ySum/num_DistributionFunc.getSumOfAllY_Values();
			
		}
		return zValues;
	}



	/*
	 * This one returns the value of the cdf at the specified x-value, with a randomized correction for the discreteness of
	 * the distribution. The fractile value for count n is defined q = P(N <= n) - rand(1)*P(N == n); 
	 */
	public double getCumulativeQuantileValue(double tmin, double tmax, double minMag, int nObserved) {
		
		this.computeNum_DistributionFunc(tmin, tmax, minMag);
		
		
		double[] xValues = num_DistributionFunc.getXVals();
		double[] yValues = num_DistributionFunc.getYVals();
		
		double ySum = 0;
		for (int i = 0; i < xValues.length; i++) {
			if (nObserved > xValues[i]) {
				ySum += yValues[i];
			} else if (nObserved == (int) (xValues[i] + 0.5)) {
				ySum += Math.random()*yValues[i];
				break;
			} else {
				break;
			}
		}

//		if(D) System.out.println(this.num_DistributionFunc + " " + this.num_DistributionFunc.getNormalizedCumDist() + " " + nObserved + " " + ySum);

		return ySum/num_DistributionFunc.getSumOfAllY_Values();
	}
	
	public static void main(String[] args) {
		// TODO Auto-generated method stub
	}



}

